{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Tutorial 7: Putting It All Together\n",
    "The purpose of this library is to cover the remaining topics which do not easily fit within another previous topic. Namely, this includes going over auto-differentiation in detail, going over GPU memory stuff, and talking about types of models that can be used for Flux.\n",
    "In this tutorial we will cover the following\n",
    " - GPU computation, requires CUDA to be installed\n",
    " - CUDA installation instructions, if needed\n",
    " - GPU movement, storage, and computation via the gpu function. [Example here](https://github.com/FluxML/Flux.jl/blob/645aa044644e36b34686286fcd12949c31a3ab68/test/cuda/cuda.jl)\n",
    " - [CuArrays Library](https://github.com/JuliaGPU/CuArrays.jl)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "We will be loading in flux!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Flux\n",
    "using Flux: throttle\n",
    "using Random\n",
    "using Test\n",
    "include(\"../src/check_cuda.jl\")\n",
    "print_system_gpu_status()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# GPU Examples\n",
    "for the next examples, you will need a working a CUDA enabled gpu\n",
    "without one, the needed libraries will not be able to build, and\n",
    "the koans will pass by default.\n",
    "You will need an NVidia graphics card to install CUDA, and unfortunately\n",
    "there are only a few macbooks that have NVideo cards (most run Radeon cards)\n",
    "However, you have a couple of options, whose exact instructions will be left\n",
    "to the reader for the sake of brevity:\n",
    " - [Google Compute Engine](https://cloud.google.com/compute/docs/gpus/) is my personal choice for renting GPU hours. You will have to set up IJulia to work over the net, but this is not too bad.\n",
    " - Run this notebook on [Google Colab](https://colab.research.google.com/drive/1zAkadxc_iJE_oj-TOdwHpfLU2oaNHJOS#scrollTo=cEOANYIVIdR5)\n",
    " - Run this locally on a computer with a GPU\n",
    "Of all the potential solutions, I would strongly recommend finding a computer with an NVIDIA GPU, or using an out of the box solution, like Colab."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# A GPU Layer koans\n",
    "To move a layer over to the GPU, we can use the helper function `Flux.gpu`\n",
    "to call it, we will use a pipe, or compose function, `|>`, which applies a\n",
    "function to the preceeding argument.\n",
    "Thus, we have `|> :: a -> (a -> b) -> b`.\n",
    "### Put that koan in your pipe!\n",
    "Using the pipe chain together `layer1` and `layer2`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "layer1 = Dense(32,16)\n",
    "layer2 = Dense(16,8)\n",
    "data = randn(MersenneTwister(42), Float32, 32)\n",
    "m = Chain(layer1, layer2)\n",
    "fn = layer2(layer1(data)) # rewrite this with pipes\n",
    "@test fn(data) == Chain(layer1, layer2)(data)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note, that our pipe construction is very close to what is going on with\n",
    "`Chain`. However, Chain gives us a nice convenience function, and allows\n",
    "us to avoid using lambda functions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Using gpu with data\n",
    "Now, we can use pipes, along with the gpu function and some behind the\n",
    "take our data x, and move it over to the gpu"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if check_cuda()\n",
    "  data = randn(MersenneTwister(42), Float32, 10)\n",
    "  data_gpu = copy(data) # modify me!\n",
    "  data_gpu = data |> gpu\n",
    "  @test typeof(data_gpu) == CuArray{Float32,1}\n",
    "else\n",
    "  print(\"CUDA library not detected. If you have a GPU please install CUDA.\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Using gpu with model layers\n",
    "scenes magic to convert the storage of layers from CPU to GPU"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if check_cuda()\n",
    "  m = Chain(Dense(81, 40), softmax)\n",
    "  pr = params(m)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "convert m params onto GPU"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "  mgpu = m # Modify me with a piped call to gpu!\n",
    "  pr_gpu = params(mgpu)\n",
    "\n",
    "  @test type(pr_gpu) != type(pr)\n",
    "else\n",
    "  print(\"CUDA library not detected. If you have a GPU please install CUDA.\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Converting Training Call to GPU\n",
    "Take th following call to `Flux.train!`, and fully convert the data used\n",
    "in training, and the model to the GPU."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m = Chain(\n",
    "    Dense(784, 32, Ïƒ),\n",
    "    Dense(32, 32),\n",
    "    Dense(32, 32),\n",
    "    Dense(32, 10),\n",
    "    softmax\n",
    "  )\n",
    "m = m # Modify me\n",
    "x = rand(784, 20) # Modify me\n",
    "x = rand(784, 20) |> gpu\n",
    "y = rand(784, 20)\n",
    "for i in 1:20 # Normalize each\n",
    "  y[:,i] = y[:,i] ./ sum(y[:,i])\n",
    "end\n",
    "\n",
    "y = copy(y) # Modify me\n",
    "data = Iterators.repeated((x, y), 50)\n",
    "loss(x, y) = Flux.mse(m(x), y)\n",
    "ps = params(m)\n",
    "\n",
    "opt = ADAM()\n",
    "evalcb = () -> @show(loss(x, y)) # Wrap this in a call to throttle\n",
    "Flux.train!(loss, ps, data, opt, cb = Flux.throttle(evalcb, 10) )\n",
    "end\n",
    "\n",
    "#= end module =#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  },
  "kernelspec": {
   "name": "julia-1.2",
   "display_name": "Julia 1.2.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
