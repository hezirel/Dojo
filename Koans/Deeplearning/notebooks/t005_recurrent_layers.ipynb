{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Tutorial 5: Recurrent Neural Networks\n",
    "\n",
    "In this tutorial we will cover the following\n",
    " - *What are Recurrent Neural Networks?* What is the fundamental problem they solve? LSTM, RNN building blocks, (aside on LTSM white papers, advanced methods like attention, word vecs, doc vecs, et cetera). Communication of RNN vs CNN differences\n",
    " - Flux RNN Layer Interface within Flux: [RNN](https://github.com/FluxML/Flux.jl/blob/master/src/layers/recurrent.jl#L70) [LSTM](https://github.com/FluxML/Flux.jl/blob/master/src/layers/recurrent.jl#L109) [GRU](https://github.com/FluxML/Flux.jl/blob/master/src/layers/recurrent.jl#L156)\n",
    " - Put it all together: [Char-Rnn, from model-zoo](https://github.com/FluxML/model-zoo/blob/master/text/char-rnn/char-rnn.jl)\n",
    " - Mention: Loss functions, training functions, sampling, character/word encoding"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Problem Statement: To Understand Flux's Recurraent NN layers\n",
    "[Backround Reading](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "[Flux Documentation](https://fluxml.ai/Flux.jl/v0.4/models/layers.html#Recurrent-Layers-1)\n",
    "# Setup\n",
    "Import the libraries here, from `Flux`, `StatsBase`, and `Base.Iterators`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ing Flux\n",
    "using Flux: onehot, chunk, batchseq, throttle, crossentropy\n",
    "using StatsBase: wsample\n",
    "using Base.Iterators: partition\n",
    "using Test\n",
    "\n",
    "text_file_local = \"../src/assets/t005_recurrent_layers/shakespeare.txt\"\n",
    "text_file_remote_url = \"https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\""
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Download shakespeare dataset\n",
    "Run this once, per time you download the project"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isfile(text_file_local) ||\n",
    "  download(text_file_remote_url,\n",
    "          text_file_local)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Set up Shakespeare dataset"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show pwd()\n",
    "text = collect(String(read(text_file_local)))\n",
    "alphabet = [unique(text)..., '_']\n",
    "text = map(ch -> onehot(ch, alphabet), text)\n",
    "stop = onehot('_', alphabet)\n",
    "\n",
    "N = length(alphabet)\n",
    "seqlen = 50\n",
    "nbatch = 50\n",
    "\n",
    "Xs = collect(partition(batchseq(chunk(text,        nbatch), stop), seqlen));\n",
    "Ys = collect(partition(batchseq(chunk(text[2:end], nbatch), stop), seqlen));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note, the following types match, for the encoding for `Xs`, a character, and\n",
    "`Ys`, the subsequent character...\n",
    "typeof(text)                                     :: Array{Flux.OneHotVector,1}\n",
    "typeof(chunk(text,nbatch))                       :: Array{Array{Flux.OneHotVector,1},1}\n",
    "typeof(batchseq(chunk(text,nbatch), stop))       :: Array{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},1}\n",
    "typeof(collect(partition(batchseq(chunk(text,nbatch), stop), seqlen))) :: Array{Array{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},1},1}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# One-Hot Encoding.\n",
    "We read our shakespeare file into a text string, and apply One-Hot text encoding on it.\n",
    "Using the variables, text, and `alphabet`, the one-hot encoding scheme, reverse the\n",
    "get a String of the characters between 100 and 117"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "idx_map = collect(100:118) # collect makes an Array from a UnitRange\n",
    "string_msg = String(['a', 'b']) # Modify me !\n",
    "@test string_msg == \"u are all resolved \""
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Working with OneHot Vectors\n",
    "Get the first letter of the dataset, from Xs"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xs_letter = \"Not a letter\" # Fix Me  !!\n",
    "xs_letter = alphabet[Xs[1][1][:,1]]\n",
    "@test alphabet[text[1]] == xs_letter"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Flux Recurrances\n",
    "Flux creates recurrant layers using the constructor, `Recur`\n",
    "In the following example, we will create a simple recurrant\n",
    "cell, for addition, what will be the result of applying it to\n",
    "the sequence of numbers from 1:10 ?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cell(h, x) = (h + x, x)\n",
    "rnn = Flux.Recur(cell, 0)\n",
    "rnn.(collect(1:100))\n",
    "\n",
    "rnn_state = 0 # Modify me !\n",
    "@test rnn.state == rnn_state"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Flux Recurance, many dimensions\n",
    "Given a `n` dimensional input, create a `Flux.Recur` cal\n",
    "modifying the code above"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 10\n",
    "seq_len = 20\n",
    "input_data = ones(n, seq_len)\n",
    "\n",
    "cell(h, x) = (h .+ x, x)\n",
    "rnn = Flux.Recur(cell, zeros(n))\n",
    "\n",
    "#= rnn.(input_data) modify the shape of the input data in this call! =#\n",
    "@test rnn.state ==  repeat([10], length(rnn.state))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note, the desired input type passed to rnn is going to be `Array{Array{Float64,1},1}``"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# RNN - Basic layers\n",
    "apply the layer to Xs[1]"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "layer = RNN(N, 1)\n",
    "layer.(Xs[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Adding Layers to Models\n",
    "Multiple Recurrent Layers can compose using our `Chain` data constructor\n",
    "and we will specify input and output dimensions of of the latent dimensions\n",
    "as the first two arguments to LSTM.\n",
    "[LSTM Source code](https://github.com/FluxML/Flux.jl/blob/master/src/layers/recurrent.jl#L101)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m = Chain(\n",
    "  LSTM(N, 128),\n",
    "  LSTM(128, 128),\n",
    "  Dense(128, N),\n",
    "  softmax\n",
    ")\n",
    "\n",
    "function loss(xs, ys)\n",
    "  l = sum(crossentropy.(m.(xs), ys))\n",
    "  Flux.truncate!(m)\n",
    "  return l\n",
    "end\n",
    "\n",
    "opt = ADAM(0.01)\n",
    "tx, ty = (Xs[5], Ys[5])\n",
    "evalcb = () -> @show loss(tx, ty)\n",
    "\n",
    "params_m = params(m)\n",
    "\n",
    "#= Fill in the first 4 args to Flux.Train =#\n",
    "#= Flux.train!(loss = NaN, params = NaN, data = NaN, opts = NaN, cb = throttle(evalcb, 30)) =#\n",
    "@test params_m != params(m)\n",
    "\n",
    "\n",
    "#= end module =#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  },
  "kernelspec": {
   "name": "julia-1.2",
   "display_name": "Julia 1.2.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
