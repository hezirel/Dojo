{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Tutorial 6: Flux Library: Optimization in Flux.jl\n",
    "The purpose of this chapter is to take a deeper look at how optimization in Flux works. Basically, we’ll dig deeper into Flux.train object, and try to understand the relationship between the three critical components data, model, and optimization routine.\n",
    "In this tutorial we will cover the following\n",
    " - [Flux.train! Function](https://github.com/FluxML/Flux.jl/blob/master/src/optimise/train.jl)\n",
    " - Flux.train arguments: loss function, model parameters, and call back\n",
    " - Inspection of each Flux.Train arguments\n",
    " - A deep dive into Flux.train is “learning” models by calling an optimizer\n",
    "\n",
    "The source code for Flux's optimization code is [available here](https://github.com/FluxML/Flux.jl/tree/master/src/optimise)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Colab Installation Commands\n",
    "If you are running this on Colab, the following commands\n",
    "need to be run first !"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
    "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
    "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
    "!apt update -q\n",
    "!apt install cuda gcc-6 g++-6 -y -q\n",
    "!ln -s /usr/bin/gcc-6 /usr/local/cuda/bin/gcc\n",
    "!ln -s /usr/bin/g++-6 /usr/local/cuda/bin/g++\n",
    "\n",
    "!curl -sSL \"https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.0-linux-x86_64.tar.gz\" -o julia.tar.gz\n",
    "!tar -xzf julia.tar.gz -C /usr --strip-components 1\n",
    "!rm -rf julia.tar.gz*\n",
    "!julia -e 'using Pkg; pkg\"add IJulia; add CuArrays; add Flux; precompile\"'"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "Load Flux"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Flux\n",
    "using Random\n",
    "using Test"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Set up some basic variables\n",
    "Note: You may need to re-initialize these variables for\n",
    "Flux.train to show a difference in parameters needed\n",
    "to pass some of the tests"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(42);\n",
    "function create_model()\n",
    "  Chain(\n",
    "    Dense(784, 32, σ),\n",
    "    Dense(32, 10), softmax)\n",
    "end\n",
    "\n",
    "m = create_model()\n",
    "loss(x, y) = Flux.mse(m(x), y)\n",
    "ps = Flux.params(m)\n",
    "\n",
    "opt = ADAM(0.1)\n",
    "x = rand(784);\n",
    "y = rand(10);\n",
    "data = Iterators.repeated((x, y), 50);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "#### Randomize model\n",
    "This is a helper function that re-initializes the model,\n",
    "allowing us to learn it many times with different staring states.\n",
    "Alternatively, we have `reinitializie_model`, which will reset\n",
    "the m to the original params.\n",
    "Prove that two `create_model` calls do not create the same\n",
    "set of parameter"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m1 = randn(1) # Fix me !\n",
    "m2 = m1 # Fix me !\n",
    "@test m1 != m2"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## More on Flux Train\n",
    "[Flux.Train] is the core of Flux's Machine Learning model\n",
    "and is responsible for updating the model parameters, via\n",
    "an loss function, to the data seen by the model\n",
    "The basic train function is as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m = create_model()\n",
    "ps_old = params(m)\n",
    "#= In this line, make a call to Flux.train! =#\n",
    "@test ps_old != ps && ps_old == ps_old"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Thus, we can see that the parameters of the model are updated!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Flux train callback functions\n",
    "Create a lambda function that accepts no input, and simply returns 0."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lambda_function = identity # Fix me!\n",
    "@test [lambda_function() for i in 1:10] ==  zeros(10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Flux.train callbacks\n",
    "In flux, we can make a call back that runs an effect, in our case,\n",
    "we will want to print out a message to IO. Write a function that"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cb = identity # Fix me\n",
    "cb = () -> println(\"Hello World!\")\n",
    "@test typeof(cb()) == Nothing"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Flux.train callbacks\n",
    "Now, we can write a useful callback function that will test our loss\n",
    "for every iteration of execution"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "test_x, test_y = x, y\n",
    "evalcb = () -> println(\"some function to report loss\")\n",
    "Flux.train!(loss, ps, data, opt, cb = evalcb)\n",
    "@test true || \"Successfully got here\""
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Callback thresholding/throttling\n",
    "Flux gives us the ability to throttle the number of times we\n",
    "invoke our callback function. This function is called\n",
    "`throttle` and it accepts a callback, and an Integer n, which\n",
    "prevent the callback from being called more than n times per second\n",
    "[Throttle Source Code Here](https://github.com/FluxML/Flux.jl/blob/master/src/utils.jl#L115)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m = create_model()\n",
    "test_x, test_y = x, y\n",
    "evalcbt = () -> @show(loss(test_x, test_y)) # Wrap this in a call to throttle\n",
    "Flux.train!(loss, ps, data, opt, cb = evalcbt)\n",
    "@test true || \"Successfully got here\"\n",
    "\n",
    "\n",
    "#= end module =#"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  },
  "kernelspec": {
   "name": "julia-1.2",
   "display_name": "Julia 1.2.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
